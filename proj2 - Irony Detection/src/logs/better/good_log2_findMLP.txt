
-----

'hidden_layer_sizes': [(20,20), (50,50), (50,50,50), (10,10)],
'activation': ['tanh'],
'alpha': [0.05, 0.0001],
'learning_rate': ['adaptive'],
'max_iter': [200, 500],
Best parameters found:
 {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.506 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.521 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.509 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.514 (+/-0.036) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.525 (+/-0.041) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.512 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.523 (+/-0.022) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.521 (+/-0.044) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.516 (+/-0.022) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.513 (+/-0.025) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.517 (+/-0.023) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.525 (+/-0.046) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.533 (+/-0.084) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.525 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.506 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.524 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
Results on the test set:
              precision    recall  f1-score   support

           0       0.66      0.66      0.66       473
           1       0.48      0.49      0.48       311

    accuracy                           0.59       784
   macro avg       0.57      0.57      0.57       784
weighted avg       0.59      0.59      0.59       784

