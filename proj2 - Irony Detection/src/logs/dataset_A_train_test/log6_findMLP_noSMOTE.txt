lower case stemming
lower case stemming
bag_of_words: 1500 max_features
bag_of_words: 1500 max_features

-----

'hidden_layer_sizes': [(10), (2), (5,5), (10,10)],
'activation': ['tanh'],
'alpha': [0.05, 0.0001],
'learning_rate': ['adaptive'],
'max_iter': [100, 200, 500],
Best parameters found:
 {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.571 (+/-0.029) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.605 (+/-0.014) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.615 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.566 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.599 (+/-0.046) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.621 (+/-0.038) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.579 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.532 (+/-0.102) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.611 (+/-0.028) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.585 (+/-0.001) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.607 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.617 (+/-0.040) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.580 (+/-0.030) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.610 (+/-0.025) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.623 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.569 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.595 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.618 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.551 (+/-0.070) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.601 (+/-0.028) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.614 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.577 (+/-0.042) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.611 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.614 (+/-0.039) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
Results on the test set:
              precision    recall  f1-score   support

           0       0.57      0.62      0.60       473
           1       0.34      0.30      0.32       311

    accuracy                           0.49       784
   macro avg       0.46      0.46      0.46       784
weighted avg       0.48      0.49      0.49       784

