lower case stemming
lower case stemming
bag_of_words: 1500 max_features
bag_of_words: 1500 max_features
Oversampling with Smote

-----

'hidden_layer_sizes': [(10), (2), (5,5), (10,10)],
'activation': ['tanh'],
'alpha': [0.05, 0.0001],
'learning_rate': ['adaptive'],
'max_iter': [100, 200, 500],
Best parameters found:
 {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.580 (+/-0.054) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.609 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.622 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.551 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.593 (+/-0.036) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.581 (+/-0.093) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.571 (+/-0.069) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.610 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.587 (+/-0.065) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.574 (+/-0.046) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.602 (+/-0.027) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.614 (+/-0.035) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.576 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.604 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.610 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.576 (+/-0.035) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.588 (+/-0.033) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.551 (+/-0.071) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 2, 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.567 (+/-0.014) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.604 (+/-0.038) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.615 (+/-0.030) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
0.584 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}
0.614 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}
0.615 (+/-0.033) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}
Results on the test set:
              precision    recall  f1-score   support

           0       0.56      0.61      0.58       473
           1       0.30      0.26      0.28       311

    accuracy                           0.47       784
   macro avg       0.43      0.43      0.43       784
weighted avg       0.46      0.47      0.46       784

